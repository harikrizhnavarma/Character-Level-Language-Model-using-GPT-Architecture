## ğŸ§  Character-Level Language Model using GPT Architecture

### ğŸ“„ Overview
This project implements a GPT-style Transformer model to generate character-level text using self-attention, trained on a custom dataset. The model learns to predict the next character in a sequence and can generate long coherent text from a given context. It is a well structed implementation enhanced with modular architecture, training evaluation, and model serialization.

### ğŸ“ Project Structure

Bigram_Language_Model/
â”‚
â”œâ”€â”€ input.txt                  # Raw training text data
â”œâ”€â”€ GPT_dataPrep.py           # Data preprocessing, encoding/decoding, batching
â”œâ”€â”€ GPT_block.py              # Transformer-based GPT model definition
â”œâ”€â”€ train_eval.py             # Training and evaluation loop
â”œâ”€â”€ generate.py               # Text generation script using the trained model
â”œâ”€â”€ hyper_parameters.py       # Centralized configuration for hyperparameters
â”œâ”€â”€ GPT.pth                   # Saved PyTorch model checkpoint
â”œâ”€â”€ GPT_output_test.txt       # Generated output text from the model

### âš™ï¸ Hyperparameters (hyper_parameters.py)

NUM_EPOCHS = 5000
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
BLOCK_SIZE = 256
NUM_EMBED = 384
NUM_HEADS = 6
NUM_LAYERS = 6
DROPOUT = 0.2
MAX_NEW_TOKENS = 10000
TRAIN_SIZE = 0.90

### ğŸ”„ Data Preparation (GPT_dataPrep.py)

Loads and encodes the character dataset from input.txt

Creates a character-level vocabulary

Implements encode, decode, and get_batch for sequence sampling

Example Output Shapes:
Input Tensor: (BATCH_SIZE, BLOCK_SIZE)

Target Tensor: (BATCH_SIZE, BLOCK_SIZE)

### ğŸ§± Model Architecture (GPT_block.py)

Key Components:
Embedding Layer â€” token + positional embeddings

Multi-Head Self-Attention â€” implemented using custom Head, MultiHeadedAttention

### ğŸ‹ï¸â€â™‚ï¸ Training Loop (train_eval.py)
Trains on batches sampled from the dataset.

Evaluates every epoch on validation data.

Saves the best model based on lowest validation loss.

Logs losses every 500 epochs.


FeedForward Layers â€” Two-layer MLP with ReLU

Residual + LayerNorm â€” for stability

Output Projection Layer â€” maps to vocab size

### ğŸ”® Text Generation (generate.py)
Loads trained model weights from GPT.pth

Accepts user input as seed text or uses newline token

Outputs generated text to GPT_output_test.txt
